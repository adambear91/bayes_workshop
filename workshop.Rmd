---
title: "Bayes Workshop"
date: "8/31/2020"
output: 
    html_document:
        toc: true
        theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
source("cake_model.R", local = TRUE)
source("email_model.R", local = TRUE)
```

## Introduction

All code for this workshop is available on [GitHub](https://github.com/adambear91/bayes_workshop). We encourage you to play around with the examples as you follow along.

Bayesian inference gives us a recipe for optimally revising uncertain beliefs after receiving some evidence. Often times, we have good models of how hidden states of the world produce different outcomes, but we want to know what those hidden states are. Bayes' rule gives us a recipe for solving such _inverse_ problems.

Consider our remarkable ability to infer what's going on in other people's minds. When a friend seems to be acting unusually cold or irritable, we can effortlessly conjure up a number of hypotheses about the cause of this behavior. Maybe my friend is angry with me, maybe he didn't sleep well, maybe he's hungry, and so on. We can do this because we have a model of how different behaviors are produced by different mental causes. I might know that when my friend is angry, he is likely to act passive aggressively, while this behavior is rare when he is happy. With a model of how observable behavior is produced by various possible hidden causes — along with baseline beliefs about the plausibility of these different causes — we can help identify the particular cause of the behavior at hand.

Bayes' theorem also gives us an elegant way to estimate coefficients in statistical models (e.g., linear regression). We'll tackle this problem in the second part of the workshop.

## Toy Examples

Here we present three toy models to help explain the basic mechanics of Bayes' theorem. 

### Coin flips

Most tutorials on probability theory and Bayesian reasoning begin with an analysis of coin flips because they are one of the simplest systems to work with. Imagine that we observe a sequence of flips of heads ($H$) or tails ($T$). We want to infer the probability $p$ that the coin will land heads on a given flip.

Although this may be a simple example for people familiar with the topic, let's be concrete about the exact inference problem at hand. What is unknown? In this case, it is a particular state of the coin: its probability of landing heads, which can take on any value from 0 (coin always lands tails) to 1 (coin always lands heads). Thus, for any probability $p$ such that $0 \leq p \leq 1$, we want to know how confident we should be that the coin lands heads with this probability.

At this point, it's easy to get confused, as our confidence is itself probablistic. That is, we are assigning a probability _to_ a probability that the coin lands heads. But these probabilities are distinct: the former represents our subjective credence in the latter, which is a state of the coin. The former is what Bayes' theorem updates. 

For each $p$, we start with a _prior_ belief that the coin lands heads with that probability: $P(\text{coin lands heads with probability } p)$. After observing some data (a sequence of coin flips), we end with a new belief, known as the _posterior_: $P(\text{coin lands heads with probability } p | D)$, where $D$ is the data and the "|" reads as "given." 

How do we correctly incorporate this new data into our beliefs? As hinted above, we must first build a model of how this data could be produced from the unknown parameters we're trying to estimate (in this case $p$). This model specifies the probability of obtaining our data given different parameter values. This quantity, known as the _likelihood_, is generally the most challenging quantity to calculate and is often the only thing that orthodox statistical methods care about. For example, the method of maximum likelihood estimation (MLE) would identity the value of $p$ that maximizes $p(D | \text{coin lands heads with probability } p)$. 

But Bayesians rightfully point out that the likelihood is not the quantity we ultimately care about. Imagine, for instance, that you've just observed two heads in a row from this unknown coin ($D=H_1H_2$). The likelihood of observing this data is $p^2$. It is easy to see that this quantity is maximized when $p=1$, i.e., when the coin is rigged to always land heads. Most of us would not jump to this conclusion as most plausible, though. Clearly, our prior belief that almost all coins in the world land heads with 50% probability exerts a large pull on our posterior beliefs, especially in a case like this in which data is limited. 

That brings us to the crux of Bayes' theorem, which gives us an easy way to integrate the prior and likelihood into the posterior — all you do is multiply:

$$posterior \propto prior*likelihood.$$
Notice we use the "$\propto$" symbol here to indicate that the posterior is _proportional to_ the product of the prior and likelihood up to a normalizing constant. In practice, this is almost always all that we'll need, as we will see later. 

Now let's come back to our example with $D=H_1H_2$. As we saw above, the likelihood in this case is just $p^2$. But to get posterior beliefs for different $p$ values, we need to specify a prior over the plausibility of these $p$ values. Here's a simple one: we assume with 95% probability that the coin is fair ($p=.5$) and with the remaining 5% probability that the coin is fully rigged to always give heads or always give tails. That is, $P(\text{coin lands heads with probability } .5) = .95$, $P(\text{coin lands heads with probability } 1) = .025$, and $P(\text{coin lands heads with probability } 0) = .025$. We assign the prior probability for any other $p$ to 0.

From Bayes' theorem, we can already see that we can discount any value of $p$ other than 0, .5, and 1, as the prior will be 0 and, therefore, the product of the prior and likelihood will yield a posterior of 0. This means we only need to evaluate the three cases where $p$ is 0, .5, or 1. Multiplying the priors from above with the likelihoods, we get (unnormalized) posteriors of $.025*0^2$, $.95*.5^2$, and $.025*1^2$, respectively. Clearly, the posterior for $p=.5$ (.238) is much larger than the posterior for $p=1$ (.025) and $p=0$ (0), matching our intuition that observing two heads from an unknown coin is insufficient evidence to overcome our prior belief that the coin is fair. Indeed, $p=.5$ is the _maximum a posteriori_ (MAP) estimate: the value that we deem most probable after observing the data.

#### Beta binomial model

Sometimes we can write out our prior in a way that greatly simplifies the math of Bayes' theorem and doesn't require computing posteriors separately for each possible $p$ value. In the case of estimating the probability of a coin's landing heads — or estimating other probabilities with this structure, such as the probability that a candidate will get a vote in a two-person election, the probability that somebody in our in-group will agree with a policy position, and so on — it is common to use a _beta distribution_ as the prior. This distribution has two positive shape parameters, $\alpha$ and $\beta$, which weigh the relative plausibility of high and low $p$ values, respectively. See below for some examples.

```{r, fig.align='center', fig.height=3, fig.width=4}
xs <- seq(.01, .99, .01)
beta1 <- c(1, 1)
beta2 <- c(3, 2)
beta3 <- c(20, 5)

tibble(
   x = xs,
   beta1 = dbeta(xs, beta1[1], beta1[2]),
   beta2 = dbeta(xs, beta2[1], beta2[2]),
   beta3 = dbeta(xs, beta3[1], beta3[2])
) %>% 
   pivot_longer(starts_with("beta"), names_to = "dist", names_prefix = "beta") %>% 
   ggplot(aes(x, value, color = dist)) +
   geom_line(size = 1, alpha = .7) +
   scale_x_continuous(expression(italic("p")), breaks = seq(0, 1, .1)) +
   scale_y_continuous("Probability", breaks = NULL) +
   scale_color_brewer(
      element_blank(), palette = "Accent", 
      labels = c(
         str_c("\u03b1 = ", beta1[1], ", \u03b2 = ", beta1[2]), 
         str_c("\u03b1 = ", beta2[1], ", \u03b2 = ", beta2[2]),
         str_c("\u03b1 = ", beta3[1], ", \u03b2 = ", beta3[2])
      )
   ) +
   theme_classic() +
   theme(text = element_text(size = 8), legend.position = "bottom") 
```
Now suppose that your data consists of observing $k$ heads out of $n$ coin flips. If your prior is beta-distributed with shape parameters $\alpha$ and $\beta$, then your posterior will also be beta-distributed, with shape parameters $\alpha + k$ and $\beta + (n - k)$. In other words, we can simply add the counts of heads and tails to our prior $\alpha$ and $\beta$ parameters, respectively.

This is a powerful trick, which you'll see exploited in many Bayesian models. Bear in mind, however, that there are many priors that cannot be represented with a beta distribution, such as the example we used above where we assigned 0 prior probability to almost all $p$ values.

### How much does Adam like chocolate cake?

Now let's switch gears to something more complicated: reading minds. Or at least reading one tiny facet of a person's mind, which is their preference for a tasty dessert. 

Imagine you have the following information. Adam is generally indifferent about eating non-chocolate desserts, but he has a sweet tooth for chocolate. Unfortunately, he's also lactose intolerant, which makes him averse to eating most foods with lactose when he doesn't have his Lactaid pills on him — though he is willing to eat foods with lactose if he absolutely loves them. You're not sure if chocolate desserts cross this threshold. (Assume for the sake of this simple example that there's no correlation between a dessert's containing chocolate and its containing lactose and also that the amount of lactose in desserts is all or none.)

Now you're out to dinner with Adam, who has pointed out that he forgot to bring Lactaid with him this evening. You run to the bathroom, and when you return, you see Adam zealously eating a chocolate dessert. You don't know whether the dessert contains lactose, but Adam does. What do you infer about the dessert's lactose content and Adam's love of chocolate desserts?

Before delving into the math of this silly example, just take a moment to think intuitively about how your beliefs might change after seeing Adam eat the dessert. Two things were initially unknown: (a) whether the dessert contains lactose and (b) how much Adam loves chocolate desserts. Presumably, if you initially thought the dessert might contain lactose, you're less likely to believe this now, since you know that Adam generally avoids foods containing lactose. At the same time, you remain open to the possibility that Adam just loves chocolate desserts so much that he was willing to incur the cost of digestive issues to enjoy this particular dessert. So, after seeing Adam eat the dessert, you simultaneously believe that it is less likely to contain lactose and more desirable to him. 

How might we formalize this? It's first worth stressing that there's almost never a 'correct' way of doing this, and any model is necessarily going to have to make certain simplifying assumptions. So as we walk through this particular formalization, think about how you might tweak certain details or build in more structure. And, most importantly, think about how you could make the example more realistic to apply it to a real research problem.

In general, for models of decision-making, it's standard to imagine that the decision-maker assigns some scalar _utility_ to different actions based on their features. By convention, the actor wants to act when the utility is positive and doesn't want to act when the utility is negative. Now recall that "Adam is generally indifferent about eating non-chocolate desserts." This is a useful (and unrealistic) clue that Adam assigns a utility of 0 to non-chocolate desserts. Meanwhile, we are told that Adam likes chocolate, suggesting that his utility from eating such a dessert is positive on its own. However, we are also told that Adam finds foods containing lactose to be aversive, suggesting that lactose lowers a food's utility. Putting it all together, we can write Adam's utility $U$ for eating desserts as

$$U = b_cC - b_lL,$$
where $C$ and $L$ are binary values set to 1 if the dessert contains chocolate and lactose, respectively, and $b_c$ and $b_l$ indicate the respective dis(utility) from eating a dessert with chocolate and lactose. For simplicity, though, we can set $b_l$ to 1, since really what we care about is Adam's love of chocolate _relative_ to the discomfort of eating a food with lactose. Therefore, the scale is arbitrary, and when $b_l=1$, $b_c$ tells us how Adam's love of chocolate compares to his hatred for lactose. If $b_c=1$, they are exactly equal, and Adam is indifferent about eating a chocolate dessert with lactose. If $b_c>1$, Adam is willing to eat the dessert despite having lactose, and vice versa for $b_c<1$. Also notice that $U=0$ for non-chocolate desserts (that don't contain lactose), correctly indicating that Adam is indifferent about eating such desserts. 

Now that we have this utility model written down, we need to convert this to a _likelihood_ function that Bayes' theorem can work with. Our "data" in this case is our observation of Adam eating a chocolate dessert. What's unknown is how much Adam loves chocolate desserts ($b_c$) and whether the dessert contains lactose ($L$). Thus, we need an expression for $P(D=\text{Adam eats chocolate dessert}|b_c,L)$.

We could simply set this probability to 1 when $U>0$, 0 when $U<0$, and .5 when $U=0$. But the real world is messy, and there are many unmodeled factors that likely add noise to Adam's decision to eat or not eat desserts (e.g., he might be more or less full from the dinner he just ate). Thus, to translate utilities into action probabilities, it is typical to pass the utility to a [logistic function](https://en.wikipedia.org/wiki/Logistic_function). (Does "logistic" ring any bells?) In essence, this function will map $U$ values near 0 to approximately .5 and gradually increase the output toward a probability of 1 as $U$ increases further (and vice versa as it decreases below $U=0$). The function takes a single parameter that controls the steepness of the transition. As you can see in the code, we choose a relatively steep curve, but you can play around with this setting yourself to see how it affects the results.

In short, with $b_l=1$ and $C=1$ (indicating that the dessert is chocolate), the utility function of Adam eating a chocolate dessert reduces to $b_c - L$. Thus, the likelihood becomes

$$P(D|b_c,L) = \text{logistic}(b_c-L).$$
Let's take a look at this function below. The probability of Adam's eating chocolate dessert based on his preference for chocolate has the characteristic "S" shape of the logistic function. However, this curve is shifted 1 unit to the right when the dessert contains lactose (dashed line), indicating that Adam's indifference point occurs when $b_c=1$, i.e., when his preference for chocolate perfectly compensates for his aversion to lactose.

```{r, fig.align='center', fig.height=3, fig.width=4}
cake_lik_plot + theme(text = element_text(size = 8))
```

So far so good. Now let's estimate what we really want: the posterior, $P(b_c,L|D)$. To do this, we first need to specify a prior, $P(b_c,L)$. Because we are jointly estimating two variables, this prior is 2-dimensional. However, if we believe that Adam's love of chocolate and the probability that the dessert has lactose are uncorrelated, we can decompose the prior into the product of the two individual terms: $P(b_c)P(L)$. For the sake of space, we won't go into detail here about how this prior is specified and instead simply show one example below. (As always, we encourage you to play around with the code!) In this example, we start with a relatively strong belief that the chocolate dessert contains lactose, and our prior on Adam's love of chocolate desserts is centered a little below 1 (left panel). That is, we start off believing that Adam is unlikely to like chocolate enough to eat a chocolate dessert that contains lactose. However, we remain open to the possibility that $b_c > 1$. 

```{r, fig.align='center', fig.height=3, fig.width=5}
cake_posterior_plot + theme(text = element_text(size = 8))
```
What happens after we observe Adam eat the chocolate dessert? Multiplying the prior by the likelihood to get a posterior, we see a shift in our beliefs in both of the directions that we intuitively predicted. Now we believe that the dessert is unlikely to contain lactose, but we also believe it's possible that it contains lactose but that Adam loves chocolate more than we initially thought (right panel).

It's worth pointing out one final quirk about this update. Recall that before we observed Adam eat the dessert, we assumed that whether the dessert has lactose and how much Adam loves chocolate were uncorrelated. But after gathering our data, these two variables have become correlated. If, for example, Adam tells us that the dessert contains lactose, this gives us information about how much he loves chocolate desserts (a lot!). Likewise, if Adam tells us he loves chocolate so much that he's willing to get sick, this gives us information about the probability that the dessert contains lactose. A cool example of a [collider](https://en.wikipedia.org/wiki/Collider_(statistics))!

### Ben's favourite professor

Ben has a favorite professor who he's been too afraid to email. One day, he works up the courage to finally send that email, hopeful that he'll receive an encouraging reply. As each day passes without a reply, though, Ben begins to wonder whether he'll ever hear back. He silently wonders to himself, "What would a Bayesian believe?"

As in the previous examples, it's helpful to first be explicit about what we're inferring. In this case, it is a probability of receiving an eventual reply from the professor. The data, however, is a bit more complex: the amount of time Ben has waited so far. This data is constantly changing, so Ben needs to continually update his belief.

As before, let's first specify the likelihoods. On the hypothesis that the professor will never reply, this is easy:

$$P(x \text{ days w/o reply}|\text{professor never responds})=1,$$
for all $x$. That is, in the case that the professor never replies, then of course the professor won't have replied by day $x$. 

What's a good model if the professor does eventually reply? Well, if we were serious about this, we might dig through our emails to see how long it took to receive replies from various professors. But we don't have time for that, so let's instead imagine that the distribution of response times follows a [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution) with a median wait time of around `r round(weib_scale*(log(2))^(1/weib_shape))` days. The particular version of this distribution that we chose also has the interesting feature that the longer you've waited for a reply, the longer you can expect to wait going forward (provided you get an eventual reply). We show this distribution below.

```{r, fig.align='center', fig.height=3, fig.width=4}
ggplot(mapping = aes(1:400, dweibull(1:400, weib_shape, weib_scale))) +
   geom_line(size = 1) +
   labs(x = "Day of Reply") +
   scale_y_continuous(element_blank(), breaks = NULL) +
   theme_classic()
```
The plot above shows us the probability of receiving a reply _on_ a given day, but for the likelihood, we want the probability of _not_ receiving a reply _after_ some number of days. We can get the probability of having received a reply prior to day $x$ from the cumulative distribution function of the Weibull distribution, $F(x)$. To get the probability of having _not_ received a reply by then, we just subtract this quantity from 1. 

Now to get our rolling posterior, $P(\text{professor never responds}|x \text{ days w/o reply})$, we just need to specify a prior. Let's assume Ben is initially optimistic: he thinks he'll receive an eventual reply with probability `r p_response` (i.e., $P(\text{professor never responds})=$ `r 1-p_response`). The trajectory of his beliefs are shown below. By day 400, he has pretty much given up all hope of ever receiving a reply.

```{r, fig.align='center', fig.height=3, fig.width=4}
posterior_simple_plot + theme(text = element_text(size = 8))
```
What if we change our hypothesis space slightly to incorporate a new hypothesis? Now instead of simply distinguishing between never receiving a reply and eventually receiving a reply, Ben considers two sub-hypotheses about why he will never receive a reply. In one case, the professor innocuously forgot to respond; in the other case, the professor willfully ignored Ben. In both of these cases, the likelihood is always 1 as before, but the priors can be different. In the example below, we show what happens if Ben starts with slightly more credence in the hypothesis that the professor will intentionally ignore him.

```{r, fig.align='center', fig.height=3, fig.width=4}
posterior_alt_plot + theme(text = element_text(size = 8))
```
Notice how even though the data can't distinguish between the "Forgot" and "Ignoring" hypotheses, the priors nevertheless diverge as the "Will respond" hypothesis becomes less plausible. This is an interesting feature of Bayesian reasoning in general: people with slightly different priors and the same evidence can end up drawing starkly different conclusions. Indeed, we could simply flip the red and green lines in the plot above and find that if Ben had assigned slightly more plausibility to the "Forgot" hypothesis relative to "Ignoring," he would've ended up concluding that the professor probably just forgot about his email.

## Outline

See [here](https://docs.google.com/document/d/1jEDeItHwcJVA60lQa6tkH0v6xUkNUUmo8nmfCbKhEdo/edit).

